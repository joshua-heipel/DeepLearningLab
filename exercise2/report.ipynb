{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Lab - Report on the Second Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joshua Heipel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matriculation number: 3706603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implementing a CNN in Tensorflow\n",
    "\n",
    "The first part of the exercise was to implement a convolutional neural network with Tensorflow. Instead of using the simple API \"Keras\" a specific framework for the tasks of this exercise was created. The implemented classes and functions contained in the \"CNN.py\" file are mainly based on the implementation of a fully connected neural network from the last exercise sheet. A pooling option is directly integrated into the convolutional layers. The output layer comes with a softmax activation function in order to calculate the loss. Training and validation of the network are implemented inside the given dummy functions of \"cnn_mnist.py\" file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning Rate\n",
    "\n",
    "In the second exercise the learning rate of the stochastic gradient descent algorithm was gradually changed. The following figure shows the learning curves (validation error) for different parameter values after 50 epochs. At a low learning rate of 0.0001 the validation error doesn't improve significantly. With increasing values (0.001 - 0.1), a greater decline can be found - with some plateaus and transient climbs in between. A high learning rate of 1.0 doesn't improve of the validation error at all. \n",
    "\n",
    "From the figure it can be concluded, that the learning rate is an important optimization parameter. At low values the convergence of the algorithm towards a minimum is immensely down immensely. At very high values the step size is too large to reduce the objective function and the algorithm doesn't converge at all. Because the minimization is calculated on the loss rather than on the validation error, the illustrated learning curves show some plateaus and temporary increases.\n",
    "\n",
    "<img src=\"results/img/learning_rate.png\" alt=\"Learning Rates\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning rates\n",
    "\n",
    "files = os.listdir(\"results/learning_rate\")\n",
    "\n",
    "learning_rate = []\n",
    "results = []\n",
    "\n",
    "for file in sorted(files):\n",
    "    with open(\"results/learning_rate/\" + file) as f:\n",
    "        data = json.load(f)\n",
    "        results += [data['learning_curve']]\n",
    "        learning_rate += [data['lr']]        \n",
    "        \n",
    "f = plt.figure()\n",
    "plt.plot(results[0])\n",
    "plt.plot(results[1])\n",
    "plt.plot(results[2])\n",
    "plt.plot(results[3])\n",
    "plt.plot(results[4])\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('validation error')\n",
    "\n",
    "plt.title(\"learning rates\")\n",
    "plt.legend(learning_rate, loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "f.savefig(\"results/img/learning_rate.png\")\n",
    "f.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convolution Type\n",
    "\n",
    "The third part of the exercise was to try out different network architectures by varying the filter sizes of the convolutional layers while keeping the rest of the configurations unchanged. The learning curves for various parameters after 50 training epochs are shown in the figure below. In all cases the validation error decreases with the number of training cycles. Regardless of the filter size, convergence is achieved after 10-15 epochs. With a size of 1x1 the validation error is about 10.5%. With larger filters it can be further reduced while the difference between the configurations becomes smaller. The lowest validation error of about 1.4% is achieved with a filter size of 7x7. As a major drawback, the runtime of the gradient descent optimization algorithm increases immensely with larger filter sizes. Therefore, large filters must be used carefully. In case of small differences between the gray values of adjacent pixels large filters shouldn't give much advantage over small filters. However, if the size is too small, the validation error becomes significantly larger.\n",
    "\n",
    "\n",
    "<img src=\"results/img/filter_size.png\" alt=\"Filter Size\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot filter sizes\n",
    "\n",
    "files = os.listdir(\"results/filter_size\")\n",
    "\n",
    "filter_size = []\n",
    "results = []\n",
    "\n",
    "for file in sorted(files):\n",
    "    with open(\"results/filter_size/\" + file) as f:\n",
    "        data = json.load(f)\n",
    "        results += [data['learning_curve']]\n",
    "        filter_size += [data['filter_size']]        \n",
    "        \n",
    "f = plt.figure()\n",
    "plt.plot(results[0])\n",
    "plt.plot(results[1])\n",
    "plt.plot(results[2])\n",
    "plt.plot(results[3])\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('validation error')\n",
    "\n",
    "plt.title(\"filter size\")\n",
    "plt.legend(filter_size, loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "f.savefig(\"results/img/filter_size.png\")\n",
    "f.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Search\n",
    "\n",
    "For the last exercise an automatic hyperparameter optimization scheme should be used to find the best configuration of the parameters learning rate, batch size, number of filters and filter size. The training and validation function as well as the configuration space are defined inside the given \"random_search.py\" file. Optimization was then run for 50 iterations with a budget of 6 training epochs each. The following figure shows the calculated losses for various randomly selected parameters. As the distribution of points indicates, the differences in the remaining loss are very high. The best result was achieved with a learning rate of 0.066 a batch size of 36, a number of 33 filters per layer and a filter size of 5x5. The remaining test error was about 2.4%.\n",
    "\n",
    "<img src=\"results/img/random_search.png\" alt=\"Random Search\" style=\"width: 400px;\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
