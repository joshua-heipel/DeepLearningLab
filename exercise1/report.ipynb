{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Report of the First exercise**: Classifying MNIST with MLPs\n",
    "\n",
    "# Joshua Heipel\n",
    "\n",
    "# matriculation number: 3706603\n",
    "\n",
    "The first exercise was about implementing a Neural Network in Python by completing the given code on https://github.com/aisrobots/dl-lab-2018. The object oriented approach containes classes for the layers (different behaviour for the first, the hidden and the last layers), the activation function and the neural network. Each of the layer classes and the activation function provide methods for forward- and backwardpropagation (fprop() and bprop()) through the network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass\n",
    "\n",
    "During a forward pass, the output of a given layer is calculated by **a = W * x + b** and **h = t(a)**, where **t** is the activation function. **a** and **t(a)** are stored for the backward pass. To calculate the output for different inputs in parallel, the numpy dot product can be used. The overall output of the network **Y_pred** for a given input **X** is then computed with the predict() method of the neural network class, which just iterates over all layers. The main functions are given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    \n",
    "    def fprop(self, input):\n",
    "        self.last_input = input\n",
    "        return self.act(input)\n",
    "\n",
    "class FullyConnectedLayer(Layer, Parameterized):\n",
    "    \n",
    "    def fprop(self, input):\n",
    "        self.last_input = input\n",
    "        a = np.dot(input, self.W) + self.b\n",
    "        t = self.activation_fun\n",
    "        h = t.fprop(a) if t else a\n",
    "        return h\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def predict(self, X):    \n",
    "        output = X\n",
    "        for layer in self.layers:\n",
    "            output = layer.fprop(output)\n",
    "        Y_pred = output\n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Pass\n",
    "\n",
    "For the backward propagation the gradient of the last layer (loss function) with respect to the input activation needs to be computed first. This is done with the input_gradient() method of the SoftmaxOutput or LinearOutput class. Then for each hidden layer the \"error_term\" (gradient with respect to its input activation) is calculated by multiplying the \"output_gradient\" of the following layer (calculated one step before) with the gradient of the Activation function. Afterwards the gradient of the parameters for the hidden layer and the new \"input_gradient\" to the previous layer (used in the next step) can be computed. The backpropagation through the whole network is done with the backpropagation() method of the neural network class, while iterating over all layers in reverse order. Again numpy dot product is used to parallelize the computation. The main functions look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    \n",
    "    def bprop(self, output_grad):\n",
    "        return output_grad * self.act_d(self.last_input)\n",
    "    \n",
    "class FullyConnectedLayer(Layer, Parameterized):\n",
    "        \n",
    "    def bprop(self, output_grad):\n",
    "        n = output_grad.shape[0]\n",
    "        t = self.activation_fun\n",
    "        # calculate the gradient with respect to the input activation\n",
    "        error_term = t.bprop(output_grad) if t else output_grad\n",
    "        # calculate the gradient of the parameters\n",
    "        self.dW = np.dot(self.last_input.transpose(), error_term) / n\n",
    "        self.db = np.mean(error_term, axis=0)\n",
    "        # calculate the input gradient to the previous laye\n",
    "        grad_input = np.dot(error_term, self.W.transpose())\n",
    "        return grad_input\n",
    "\n",
    "class LinearOutput(Layer, Loss):\n",
    "    \n",
    "    def input_grad(self, Y, Y_pred):           \n",
    "        return Y_pred - Y\n",
    "\n",
    "class SoftmaxOutput(Layer, Loss):\n",
    "    \n",
    "    def input_grad(self, Y, Y_pred):        \n",
    "        return Y_pred - Y\n",
    "    \n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def backpropagate(self, Y, Y_pred, upto=0):\n",
    "        next_grad = self.layers[-1].input_grad(Y, Y_pred)\n",
    "        output = next_grad\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            output = layer.bprop(output)  \n",
    "        grad = output\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "After all to train the network for given dataset (X,Y) the (stochastic) gradient descent methods of the neural network class are used. During each cycle, both, the stochastic version (sgd_epoch() method) and the standard gradient descent (gd_epoch() method), partitionate the given dataset into mini-batches first. Then for each mini-batch a forward and backward pass is computed. While the stochastic version updates the parameters of the layers directly during each forward-backward pass, the gradients of all parameters in the standard version are accumulated over all mini-batches before updating. Training of the network is done by a given number of update-cycles (epoches). Both gradient descent methods are found in the Jupyter Notebook document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of meta parameters\n",
    "\n",
    "Finally the implementation should be used to classify handwritten digits of the MNIST dataset. It was found, that the given parameter setting already leeds to a classification error on the validation dataset of below 3%. Changing parameters (number of units per layer, different activation functions, parameter initialization, learning rate, batch size and number of training epoches) as well as adding more layers could improve the resulting validation error just a little. So the final setting doesn't differ from the original setting much. The illustration shows, that the loss on the training data as well as the classification error on the training and test data is still improving towards the end of the update-cycles, while the loss on the validation data has already saturated very early."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
